---
title: Hive1.2的部署
date: 2015-11-04 22:55:26
tags:
- 大数据
- 集群
- Hive
---

# Hive1.2的部署

## 简介
hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。

听起来HBase与Hive有些类似，概念也有点模糊，那我们先了解下他们之间有什么区别：

HBase是一种分布式、面向列的NoSQL数据库，基于HDFS存储，以表的形式存储数据，表由行和列组成，列划分到列族中。HBase不提供类SQL查询语言，要想像SQL这样查询数据，可以使用Phonix，让SQL查询转换成hbase的扫描和对应的操作，也可以使用现在说讲Hive仓库工具，让HBase作为Hive存储。

Hive是运行在Hadoop之上的数据仓库，将结构化的数据文件映射为一张数据库表，提供简单类SQL查询语言，称为HQL，并将SQL语句转换成MapReduce任务运算。有利于利用SQL语言查询、分析数据，适于处理不频繁变动的数据。Hive底层可以是HBase或者HDFS存储的文件。

两者都是基于Hadoop上不同的技术，相互结合使用，可处理企业中不同类型的业务，利用Hive处理非结构化离线分析统计，利用HBase处理在线查询。

由于HDFS不能存储元数据,因此Hive需要一个辅助的关系数据库来保存它的元数据信息.
可以使用它本地的derby数据库,也可以使用本地或远程的MYSQL数据库来存储.通常我们都是存放在MYSQL中的.

<!--more-->

## 部署
由于Hive是运行在Hadoop之上的数据仓库.它的所有数据都是存放在Hadoop兼容的存储上的.因此它本身的集群其实就是启动N多个相同的Hive节点就可以了.也就是说,我们可以完全安装修改一台的Hive.然后直接把整个Hive文件夹拷贝到其他机器上即可.

1. 在MYSQL中创建一个独立的database.命名为`hive`
2. 解压`apache-hive-1.2.1-bin.tar.gz`到/home/hadoop/下 然后改名为`hive`
3. 修改环境变量  
	
	```bash
	export HADOOP_HOME=/home/hadoop/hadoop  
	export HIVE_HOME=/home/hadoop/hive  
	export PATH=$PATH:$HIVE_HOME/bin  
	export CLASS_PATH=$CALSSPATH:$HIVE_HOME/lib  
	```
4. 在`hive/conf`下 拷贝 `hive_env.sh.template` 改名成 `hive_env.sh`
5. 修改这个文件里面的HADOOP_HOME和HIVE_CONF_DIR

	```bash
	#Set HADOOP_HOME to point to a specific hadoop install directory
	export HADOOP_HOME=/home/hadoop/hadoop-2.6.0
 	#Hive Configuration Directory can be controlled by:
	export HIVE_CONF_DIR=/home/hadoop/apache-hive-1.2.0-bin/conf
	```
6. 在hive/conf下 拷贝 hive-default.xml.template 改名成 hive-site.xml
7. 修改里面的所有内容为:(因为会先自动的加载hive-default.xml里面的所有内容,所以hive-site里面的内容就是自定义的配置)

	```xml
	<configuration>
    <property> 
        <name>javax.jdo.option.ConnectionURL</name> 
        <value>jdbc:mysql://10.211.55.2:3306/hive</value> 
    </property> 
    <property> 
        <name>javax.jdo.option.ConnectionDriverName</name> 
        <value>com.mysql.jdbc.Driver</value> 
        <description>驱动名</description> 
    </property> 
    <property> 
        <name>javax.jdo.option.ConnectionUserName</name> 
        <value>root</value> 
        <description>用户名</description> 
    </property> 
    <property> 
        <name>javax.jdo.option.ConnectionPassword</name> 
        <value>admin</value> 
        <description>密码</description> 
    </property> 
    <property> 
        <name>hive.metastore.warehouse.dir</name> 
        <value>/home/hadoop/hive/warehouse</value> 
        <description>数据路径（相对hdfs）</description> 
    </property>
	</configuration>
	```
8. 放入mysql的驱动到 hive/lib中 比如: `mysql-connector-java-5.1.31-bin.jar`
9. 找到一个叫`jline-2.12.jar`的文件，复制他，去hadoop主目录将`hadoop/yarn/lib`下的`jline0.9.94.jar`替换成刚刚复制的,否则会出现包冲突.
10. 切换到hive/bin,输入 `bin/hive --service metastore`
	当显示`Starting Hive Metastore Server`就表示启动成功了.
	
## 验证
启动以后,在终端中输入`JPS`.可以 看到一个 `RunJar`的服务,这就表示启动成功了.

然后再输入`./hive`可以进入hive的命令行.

```bash
[root@HMaster0 ~]# hive
Logging initialized usingconfiguration in file:/opt/apache-hive-1.2.0-bin/conf/hive-log4j.properties
hive> show databases;
OK
default
Time taken: 0.986 seconds,Fetched: 1 row(s)
```

1. 创建一个测试库

	```bash
	hive> create database test;
	```
2. 创建一个hive_test的表,并指定字段分隔符为tab建

	```bash
	hive> create table hive_test(id int,name string) row format delimited fields terminated by '\t';
	```

3. 从本地文件中导入数据到Hive表中

	```bash
	hive> load data local inpath '/home/hadoop/hive/examples/files/kv1.txt' overwrite into table hive_test;
	```

4. 执行最简单的查询

	```bash
	hive> select * from hive_test;
	```
	
到此,Hive的最简单的安装就结束了.


